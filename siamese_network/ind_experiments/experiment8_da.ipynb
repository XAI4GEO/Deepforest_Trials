{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def _create_model(nneurons, nfilters, ndropout, npool):\n",
    "    inputs = keras.Input((100, 100, 3))\n",
    "    x = keras.layers.Conv2D(nneurons[0], (nfilters[0], nfilters[0]), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(npool[0], npool[0]), data_format='channels_last')(x)\n",
    "    x = keras.layers.Dropout(ndropout[0])(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(nneurons[1], (nfilters[1], nfilters[1]), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(npool[1], npool[1]), data_format='channels_last')(x)\n",
    "    x = keras.layers.Dropout(ndropout[0])(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(nneurons[2], (nfilters[2], nfilters[2]), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(npool[2], npool[2]), data_format='channels_last')(x)\n",
    "    x = keras.layers.Dropout(ndropout[0])(x)\n",
    "\n",
    "    pooledOutput = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    pooledOutput = keras.layers.Dense(nneurons[3])(pooledOutput)\n",
    "    outputs = keras.layers.Dense(nneurons[4])(pooledOutput)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def _euclidean_distance(vectors):\n",
    "    (featA, featB) = vectors\n",
    "    sum_squared = k.sum(k.square(featA - featB), axis=1, keepdims=True)\n",
    "    return k.sqrt(k.maximum(sum_squared, k.epsilon()))\n",
    "\n",
    "def siamese_model(nneurons, nfilters, ndropout, npool):\n",
    "    feature_extractor_model = _create_model(nneurons, nfilters, ndropout, npool)\n",
    "    imgA = keras.Input(shape=(100, 100, 3))\n",
    "    imgB = keras.Input(shape=(100, 100, 3))\n",
    "    featA = feature_extractor_model(imgA)\n",
    "    featB = feature_extractor_model(imgB)\n",
    "    distance = keras.layers.Lambda(_euclidean_distance)([featA, featB])\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "    model = keras.Model(inputs=[imgA, imgB], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def compile_model(model, lr, metrics):\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    metrics = metrics\n",
    "    model.compile(loss=loss, optimizer=opt, metrics=metrics)\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, batch_size, epochs):\n",
    "    callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, start_from_epoch=20, restore_best_weights=True)\n",
    "    history = model.fit([X_train[:, 0], X_train[:, 1]], y_train[:], validation_data=([X_val[:,0], X_val[:,1]], y_val[:]), batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
    "    return history\n",
    "\n",
    "def predict_model(model, images_pair):\n",
    "    labels_pred = model.predict([images_pair[:, 0], images_pair[:, 1]])\n",
    "    return labels_pred\n",
    "\n",
    "def plots(history, labels_pred, labels_pair, filename):\n",
    "    _plot_history(history, ['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])\n",
    "    _plot_labels(labels_pred, labels_pair)\n",
    "    _plot_histogram(labels_pred, labels_pair)\n",
    "    save_image(filename) \n",
    "\n",
    "def _plot_history(history, metrics):\n",
    "    \"\"\"\n",
    "    Plot the training history\n",
    "\n",
    "    Args:\n",
    "        history (keras History object that is returned by model.fit())\n",
    "        metrics (str, list): Metric or a list of metrics to plot\n",
    "    \"\"\"\n",
    "    fig1 = plt.figure()\n",
    "    history_df = pd.DataFrame.from_dict(history.history)\n",
    "    plt.plot(history_df[metrics], label=metrics)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"metric\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "def _plot_labels(labels_pred, labels_pair):\n",
    "    fig2 = plt.figure(figsize=(10, 5))\n",
    "    sub = fig2.add_subplot(1, 2, 1)\n",
    "    mask = np.isin(labels_pair, 0)\n",
    "    sub.plot(labels_pred[mask])\n",
    "    sub.plot(labels_pair[mask])\n",
    "    sub.set_xlabel('Epochs')\n",
    "    sub.set_ylabel('Similarity Value')\n",
    "    sub.set_title('Dissimilar Pairs')\n",
    "\n",
    "    sub = fig2.add_subplot(1, 2, 2)\n",
    "    mask = np.isin(labels_pair, 1)\n",
    "    sub.plot(labels_pred[mask])\n",
    "    sub.plot(labels_pair[mask])\n",
    "    sub.set_xlabel('Epochs')\n",
    "    sub.set_ylabel('Similarity Value')\n",
    "    sub.set_title('Similar Pairs')\n",
    "\n",
    "def _plot_histogram(labels_pred, labels_pair):\n",
    "    fig3 = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    mask = np.isin(labels_pair, 0)\n",
    "    counts, bins = np.histogram(labels_pred[mask])\n",
    "    sub = fig3.add_subplot(1, 2, 1)\n",
    "    sub.stairs(counts, bins)\n",
    "    sub.set_xlabel('Similarity Value')\n",
    "    sub.set_ylabel('Count')\n",
    "    sub.set_title('Dissimilar Pairs')\n",
    "\n",
    "    mask = np.isin(labels_pair, 1)\n",
    "    sub = fig3.add_subplot(1, 2, 2)\n",
    "    counts, bins = np.histogram(labels_pred[mask])\n",
    "    sub.stairs(counts, bins)\n",
    "    sub.set_xlabel('Similarity Value')\n",
    "    sub.set_ylabel('Count')\n",
    "    sub.set_title('Similar Pairs')\n",
    "\n",
    "def save_image(filename):\n",
    "    \n",
    "    # PdfPages is a wrapper around pdf \n",
    "    # file so there is no clash and\n",
    "    # create files with no error.\n",
    "    p = PdfPages(filename)\n",
    "      \n",
    "    # get_fignums Return list of existing\n",
    "    # figure numbers\n",
    "    fig_nums = plt.get_fignums()  \n",
    "    figs = [plt.figure(n) for n in fig_nums]\n",
    "      \n",
    "    # iterating over the numbers in list\n",
    "    for fig in figs: \n",
    "        \n",
    "        # and saving the files\n",
    "        fig.savefig(p, format='pdf') \n",
    "          \n",
    "    # close the object\n",
    "    p.close()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from rioxarray.exceptions import NoDataInBounds\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "import IDTreeS_dataset\n",
    "\n",
    "# Data Loader from IDTreeS Dataset\n",
    "rgb_paths = glob.glob('../../train/RemoteSensing/RGB/*.tif')\n",
    "bboxes_paths = glob.glob('../../train/ITC/train_*.shp')\n",
    "classes_path = '../../train/Field/train_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert generator dataset in to suitable input for keras\n",
    "ds = IDTreeS_dataset.IDTreeSDataset(rgb_paths, bboxes_paths, classes_path, augment_data=False)\n",
    "\n",
    "ids, Y_orig, X_orig = ds.get_cutouts()\n",
    "\n",
    "nclasses = len(np.unique(Y_orig)) #Number of classes in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, Y):\n",
    "    classes = [0, 4, 8]\n",
    "    mask = np.isin(Y, classes)\n",
    "    X_red = X[mask]\n",
    "    Y_red = Y[mask]\n",
    "\n",
    "    X_pp = []\n",
    "    Y_pp = []\n",
    "    for i in range(len(classes)):\n",
    "        mask = np.isin(Y_red, classes[i])\n",
    "        Y_red[mask] = i\n",
    "        X_pp.append(X_red[mask][:30])\n",
    "        Y_pp.append(Y_red[mask][:30])\n",
    "\n",
    "    return np.array(X_pp), np.array(Y_pp)\n",
    "\n",
    "X, Y = preprocess_data(X_orig, Y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X.transpose(0,1,3,4,2)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8100,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_pair, labels_pair = generate_train_image_pairs(X, Y)\n",
    "labels_pair.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and validation data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_pair, labels_pair, test_size=0.2, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.transpose([0,1,3,4,2])\n",
    "X_val = X_val.transpose([0,1,3,4,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('train_data_da.npz', X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('train_data_pp.npz', X=X, y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Base Parameters\n",
      "Epoch 1/10\n",
      "102/102 [==============================] - 104s 986ms/step - loss: 2.3447 - binary_accuracy: 0.3429 - val_loss: 0.6815 - val_binary_accuracy: 0.7049\n",
      "Epoch 2/10\n",
      "102/102 [==============================] - 106s 1s/step - loss: 0.6839 - binary_accuracy: 0.6179 - val_loss: 0.6691 - val_binary_accuracy: 0.7049\n",
      "Epoch 3/10\n",
      "102/102 [==============================] - 119s 1s/step - loss: 0.6725 - binary_accuracy: 0.6571 - val_loss: 0.6599 - val_binary_accuracy: 0.7049\n",
      "Epoch 4/10\n",
      "102/102 [==============================] - 115s 1s/step - loss: 0.6661 - binary_accuracy: 0.6571 - val_loss: 0.6517 - val_binary_accuracy: 0.7049\n",
      "Epoch 5/10\n",
      "102/102 [==============================] - 117s 1s/step - loss: 0.6607 - binary_accuracy: 0.6571 - val_loss: 0.6443 - val_binary_accuracy: 0.7049\n",
      "Epoch 6/10\n",
      "102/102 [==============================] - 109s 1s/step - loss: 0.6563 - binary_accuracy: 0.6571 - val_loss: 0.6382 - val_binary_accuracy: 0.7049\n",
      "Epoch 7/10\n",
      "102/102 [==============================] - 110s 1s/step - loss: 0.6528 - binary_accuracy: 0.6571 - val_loss: 0.6327 - val_binary_accuracy: 0.7049\n",
      "Epoch 8/10\n",
      "102/102 [==============================] - 112s 1s/step - loss: 0.6501 - binary_accuracy: 0.6571 - val_loss: 0.6285 - val_binary_accuracy: 0.7049\n",
      "Epoch 9/10\n",
      "102/102 [==============================] - 103s 1s/step - loss: 0.6479 - binary_accuracy: 0.6571 - val_loss: 0.6250 - val_binary_accuracy: 0.7049\n",
      "Epoch 10/10\n",
      "102/102 [==============================] - 105s 1s/step - loss: 0.6464 - binary_accuracy: 0.6571 - val_loss: 0.6222 - val_binary_accuracy: 0.7049\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "#Base parameters\n",
    "filename = 'trial.pdf'\n",
    "nneurons = [32, 64, 96, 64, 32]\n",
    "nfilters = [5, 5, 5]\n",
    "ndropout = [0.4, 0.4, 0.4]\n",
    "npool = [2, 2, 2]\n",
    "lr = 0.001 \n",
    "batchsize = 64\n",
    "epochs = 10\n",
    "print ('Created Base Parameters')\n",
    "\n",
    "#Load data\n",
    "# X_train, y_train, X_val, y_val = read_data('train_data.npz')\n",
    "# print ('Loaded Data')\n",
    "\n",
    "#Create model\n",
    "model = siamese_model(nneurons, nfilters, ndropout, npool)\n",
    "# load_model_weights(model, 'weights_base.h5')\n",
    "# print ('Created Model')\n",
    "\n",
    "#Compile model\n",
    "metrics = [keras.metrics.BinaryAccuracy(threshold=0.5)]\n",
    "compile_model(model, lr, metrics)\n",
    "# print ('Compiled Model')\n",
    "\n",
    "#Train data\n",
    "history = train_model(model, X_train, y_train, X_val, y_val, batchsize, epochs)\n",
    "# print ('Trained Model')\n",
    "\n",
    "#Model Prediction\n",
    "images_pair = np.append(X_train, X_val, axis = 0)\n",
    "labels_pred = predict_model(model, images_pair)\n",
    "labels_pair = np.append(y_train, y_val, axis = 0)\n",
    "print ('Model Prediction Completed')\n",
    "\n",
    "#Plot data\n",
    "plots(history, labels_pred, labels_pair, filename)\n",
    "print ('Created Plots')\n",
    "\n",
    "print (history.history)\n",
    "print ('Best Accuracy on Validation Set: ',max(history.history['val_binary_accuracy']))\n",
    "\n",
    "# model.save_weights('optimized_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai4geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
